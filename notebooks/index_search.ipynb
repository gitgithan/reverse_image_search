{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory(index):\n",
    "    # write index to file\n",
    "    faiss.write_index(index, './temp.index')\n",
    "    # get file size\n",
    "    file_size = os.path.getsize('./temp.index')\n",
    "    # delete saved index\n",
    "    os.remove('./temp.index')\n",
    "    print(f\"File size: {file_size/1024**2} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = pickle.load(open('features/features-caltech101-resnet.pickle','rb'))\n",
    "# feature_list = feature_list/np.linalg.norm(feature_list,axis=1).reshape(-1,1)  # normalize features so each column has length 1\n",
    "pca = PCA(n_components=128)\n",
    "feature_list_compressed = pca.fit_transform(feature_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flat (Ground Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 71.43754291534424 MB\n",
      "File size: 4.464886665344238 MB\n"
     ]
    }
   ],
   "source": [
    "index = faiss.IndexFlatL2(2048)\n",
    "index.train(feature_list)\n",
    "index.add(feature_list)\n",
    "\n",
    "index_compressed = faiss.IndexFlatL2(128)\n",
    "index_compressed.train(feature_list_compressed)\n",
    "index_compressed.add(feature_list_compressed)\n",
    "\n",
    "get_memory(index)\n",
    "get_memory(index_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IVFFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 72.28940868377686 MB\n",
      "File size: 4.5843305587768555 MB\n"
     ]
    }
   ],
   "source": [
    "index_ivf = faiss.index_factory(2048, 'IVF100,Flat')\n",
    "index_ivf.train(feature_list)\n",
    "index_ivf.add(feature_list)\n",
    "\n",
    "index_ivf_compressed = faiss.index_factory(128, 'IVF100,Flat')\n",
    "index_ivf_compressed.train(feature_list_compressed)\n",
    "index_ivf_compressed.add(feature_list_compressed)\n",
    "\n",
    "\n",
    "get_memory(index_ivf)\n",
    "get_memory(index_ivf_compressed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IVFPQ\n",
    "- 30x memory savings compared to IVFFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 2.921710968017578 MB\n",
      "File size: 0.3142890930175781 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    }
   ],
   "source": [
    "nlist = 100\n",
    "m = 8\n",
    "nbits=8\n",
    "\n",
    "quantizer = faiss.IndexFlatL2(2048)  # this remains the same\n",
    "index_ivfpq = faiss.IndexIVFPQ(quantizer, 2048, nlist, m, nbits)\n",
    "# index_ivfpq = faiss.index_factory(2048, 'IVF100,PQ8')\n",
    "index_ivfpq.train(feature_list)\n",
    "index_ivfpq.add(feature_list)\n",
    "\n",
    "quantizer = faiss.IndexFlatL2(128)  # this remains the same\n",
    "index_ivfpq_compressed = faiss.IndexIVFPQ(quantizer, 128, nlist, m, nbits)\n",
    "# index_ivfpq_compressed = faiss.index_factory(128, 'IVF100,PQ8')\n",
    "index_ivfpq_compressed.train(feature_list_compressed)\n",
    "index_ivfpq_compressed.add(feature_list_compressed)\n",
    "\n",
    "get_memory(index_ivfpq)  \n",
    "get_memory(index_ivfpq_compressed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPQIVFPQ\n",
    "- Extremely slow for large dimensions  (https://github.com/facebookresearch/faiss/issues/625)\n",
    "- Interrupted kernel at 4 mins for 2048 dimensions, only attempting 128 dimensions for OPQ\n",
    "- Slightly larger index than IVFPQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 0.37685680389404297 MB\n"
     ]
    }
   ],
   "source": [
    "index_opqivfpq_compressed = faiss.index_factory(128, 'OPQ8,IVF100,PQ8')\n",
    "index_opqivfpq_compressed.train(feature_list_compressed)\n",
    "index_opqivfpq_compressed.add(feature_list_compressed)\n",
    "\n",
    "get_memory(index_opqivfpq_compressed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_indices = random.sample(range(0, len(feature_list)), 100)\n",
    "random_feature_list = np.array([feature_list[each_index] for each_index in random_image_indices])\n",
    "random_feature_list_compressed = np.array([feature_list_compressed[each_index] for each_index in random_image_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground truth distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_flat, I_flat = index.search(random_feature_list, 6)\n",
    "D_flat_compressed, I_flat_compressed = index_compressed.search(random_feature_list_compressed, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing indexes against ground truth\n",
    "- IVF 100 is very high, close to known number of clusters at ~101, which helps recall\n",
    "- IVFFlat is almost same size as IVFFlat (compressed) but slightly higher recall\n",
    "- Strangely IVFPQ compressed from 2048 to 128 dimensions increases recall@5 0.44 to 0.66\n",
    "- When trying to improve IVFPQ, lowering nbits from 8 to 7 to address the warnings leaves exact same 0.44 recall on IVFPQ and worsens IVFPQ compressed from 0.66 to 0.6 -> may be good to ignore warnings\n",
    "- For this dataset, IVFPQ has 30x smaller memory footprint than IVFFlat but only slightly lower recall 0.66 vs 0.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(I_gt,feature_list,index, name=''):\n",
    "    D, I = index.search(feature_list, 6)\n",
    "    print(f'Recall@5 for {name}: {(I[:,1:6] == I_gt[:,[1]]).sum()/len(feature_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 for IVFFlat: 0.8\n",
      "Recall@5 for IVFPQ: 0.44\n",
      "Recall@5 for IVFFlat (compressed): 0.79\n",
      "Recall@5 for IVFPQ (compressed): 0.66\n",
      "Recall@5 for OPQIVFPQ (compressed): 0.71\n"
     ]
    }
   ],
   "source": [
    "index_names = ['IVFFlat','IVFPQ']\n",
    "index_names_compressed = ['IVFFlat (compressed)','IVFPQ (compressed)','OPQIVFPQ (compressed)']\n",
    "\n",
    "indexes = [index_ivf,\n",
    "           index_ivfpq]\n",
    "\n",
    "indexes_compressed = [index_ivf_compressed,\n",
    "                      index_ivfpq_compressed,\n",
    "                      index_opqivfpq_compressed]\n",
    "\n",
    "for name, idx in zip(index_names, indexes):\n",
    "    compare(I_flat,random_feature_list,idx, name)\n",
    "    \n",
    "    \n",
    "for name, idx in zip(index_names_compressed, indexes_compressed):\n",
    "    compare(I_flat_compressed,random_feature_list_compressed,idx, name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking what lowers recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 for : 0.66\n"
     ]
    }
   ],
   "source": [
    "index_ivf_compressed = faiss.index_factory(128, 'IVF100,PQ8')\n",
    "index_ivf_compressed.train(feature_list_compressed)\n",
    "index_ivf_compressed.add(feature_list_compressed)\n",
    "\n",
    "compare(I_flat_compressed,random_feature_list_compressed,index_ivf_compressed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "- Lower nlist 100 to 50\n",
    "- Lower PQ M 8 to 2\n",
    "\n",
    "Results\n",
    "- nlist of IVF has much larger impact on recall@5 than nbits of PQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 for : 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 for : 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 for : 0.36\n"
     ]
    }
   ],
   "source": [
    "index_ivfpq_compressed = faiss.index_factory(128, 'IVF100,PQ2')\n",
    "index_ivfpq_compressed.train(feature_list_compressed)\n",
    "index_ivfpq_compressed.add(feature_list_compressed)\n",
    "\n",
    "compare(I_flat_compressed,random_feature_list_compressed,index_ivfpq_compressed)\n",
    "\n",
    "index_ivfpq_compressed = faiss.index_factory(128, 'IVF50,PQ8')\n",
    "index_ivfpq_compressed.train(feature_list_compressed)\n",
    "index_ivfpq_compressed.add(feature_list_compressed)\n",
    "\n",
    "compare(I_flat_compressed,random_feature_list_compressed,index_ivfpq_compressed)\n",
    "\n",
    "index_ivfpq_compressed = faiss.index_factory(128, 'IVF50,PQ2')\n",
    "index_ivfpq_compressed.train(feature_list_compressed)\n",
    "index_ivfpq_compressed.add(feature_list_compressed)\n",
    "\n",
    "compare(I_flat_compressed,random_feature_list_compressed,index_ivfpq_compressed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom IndexPQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "class CustomIndexPQ:\n",
    "    \n",
    "    BITS2DTYPE = {\n",
    "        8: np.uint8,\n",
    "        16: np.uint16,\n",
    "    }\n",
    "    \n",
    "    def __init__(self,d: int,m: int,nbits: int,) -> None:\n",
    "        \"\"\"Custom IndexPQ implementation.\n",
    "        Parameters\n",
    "        ----------\n",
    "        d\n",
    "            Dimensionality of the original vectors.\n",
    "        m\n",
    "            Number of segments.\n",
    "        nbits\n",
    "            Number of bits.\n",
    "        \"\"\"\n",
    "        if d % m != 0:\n",
    "            raise ValueError(\"d needs to be a multiple of m\")\n",
    "\n",
    "        if nbits not in CustomIndexPQ.BITS2DTYPE:\n",
    "            raise ValueError(f\"Unsupported number of bits {nbits}\")\n",
    "\n",
    "        self.m = m\n",
    "        self.k = 2**nbits\n",
    "        self.d = d\n",
    "        self.ds = d // m\n",
    "\n",
    "        self.estimators = [KMeans(n_clusters=self.k, random_state=1) for _ in range(m)]\n",
    "\n",
    "        self.is_trained = False\n",
    "\n",
    "        self.dtype = CustomIndexPQ.BITS2DTYPE[nbits]\n",
    "        self.dtype_orig = np.float32\n",
    "\n",
    "    def train(self, X: np.ndarray) -> None:\n",
    "        \"\"\"Train M KMeans estimators.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X\n",
    "            Array of shape `(n, d)` and dtype `float32`.\n",
    "        \"\"\"\n",
    "        for i in range(self.m):\n",
    "            estimator = self.estimators[i]\n",
    "            X_i = X[:, i * self.ds : (i + 1) * self.ds]\n",
    "            estimator.fit(X_i)\n",
    "\n",
    "        self.is_trained = True\n",
    "\n",
    "\n",
    "    def encode(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Encode original features into codes.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X\n",
    "            Array of shape `(n_queries, d)` of dtype `np.float32`.\n",
    "        Returns\n",
    "        -------\n",
    "        result\n",
    "            Array of shape `(n_queries, m)` of dtype `np.uint8`.\n",
    "        \"\"\"\n",
    "    \n",
    "        n = len(X)\n",
    "        result = np.empty((n, self.m), dtype=self.dtype)  #Prevents automatic 'float64' causing IndexError: arrays used as indices must be of integer (or boolean) type\n",
    "\n",
    "        for i in range(self.m):\n",
    "            estimator = self.estimators[i]\n",
    "            X_i = X[:, i * self.ds : (i + 1) * self.ds]\n",
    "            result[:, i] = estimator.predict(X_i)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def add(self, X: np.ndarray) -> None:\n",
    "        \"\"\"Add vectors to the database (their encoded versions).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X\n",
    "            Array of shape `(n_codes, d)` of dtype `np.float32`.\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"The quantizer needs to be trained first.\")\n",
    "        self.codes = self.encode(X)\n",
    "\n",
    "    def compute_asymmetric_distances(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute asymmetric distances to all database codes.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X\n",
    "            Array of shape `(n_queries, d)` of dtype `np.float32`.\n",
    "        Returns\n",
    "        -------\n",
    "        distances\n",
    "            Array of shape `(n_queries, n_codes)` of dtype `np.float32`.\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"The quantizer needs to be trained first.\")\n",
    "\n",
    "        if self.codes is None:\n",
    "            raise ValueError(\"No codes detected. You need to run `add` first\")\n",
    "\n",
    "        n_queries = len(X)\n",
    "        n_codes = len(self.codes)\n",
    "\n",
    "        distance_table = np.empty(\n",
    "            (n_queries, self.m, self.k), dtype=self.dtype_orig\n",
    "        )  # (n_queries, m, k)\n",
    "\n",
    "        for i in range(self.m):\n",
    "            X_i = X[:, i * self.ds : (i + 1) * self.ds]  # (n_queries, ds)\n",
    "            centers = self.estimators[i].cluster_centers_  # (k, ds)\n",
    "            distance_table[:, i, :] = euclidean_distances(X_i, \n",
    "                                                          centers, \n",
    "                                                          squared=True)\n",
    "\n",
    "        distances = np.zeros((n_queries, n_codes), dtype=self.dtype_orig)\n",
    "\n",
    "        for i in range(self.m):\n",
    "            distances += distance_table[:, i, self.codes[:, i]]\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def search(self, X: np.ndarray, k: int) -> tuple:\n",
    "        \"\"\"Find k closest database codes to given queries.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X\n",
    "            Array of shape `(n_queries, d)` of dtype `np.float32`.\n",
    "        k\n",
    "            The number of closest codes to look for.\n",
    "        Returns\n",
    "        -------\n",
    "        distances\n",
    "            Array of shape `(n_queries, k)`.\n",
    "        indices\n",
    "            Array of shape `(n_queries, k)`.\n",
    "        \"\"\"\n",
    "        n_queries = len(X)\n",
    "        distances_all = self.compute_asymmetric_distances(X)\n",
    "\n",
    "        indices = np.argsort(distances_all, axis=1)[:, :k]\n",
    "\n",
    "        distances = np.empty((n_queries, k), dtype=np.float32)\n",
    "        for i in range(n_queries):\n",
    "            distances[i] = distances_all[i][indices[i]]\n",
    "\n",
    "        return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, m, nbits = 128,8,8\n",
    "custom = CustomIndexPQ(d, m, nbits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "custom.train(feature_list_compressed)\n",
    "custom.add(feature_list_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[10527.352 , 13998.237 , 15301.726 , 15498.508 , 15772.971 ],\n",
       "        [ 9672.746 , 12653.403 , 12746.408 , 12782.03  , 13135.024 ],\n",
       "        [ 8957.763 , 13945.396 , 14109.893 , 14781.04  , 14956.603 ],\n",
       "        [14499.895 , 16428.25  , 17053.215 , 21339.955 , 23001.535 ],\n",
       "        [11837.461 , 13613.828 , 14375.717 , 14442.3955, 14513.7705]],\n",
       "       dtype=float32),\n",
       " array([[8115, 8131, 8156, 8166, 8117],\n",
       "        [7977, 7983, 8028, 8003, 8001],\n",
       "        [ 583,  586,  582,  707,  588],\n",
       "        [7474, 7472, 7473, 7483, 7503],\n",
       "        [5807, 5803, 5846, 5849, 5865]]))"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_custom, I_custom = custom.search(random_feature_list_compressed, 5)\n",
    "D_custom[:5], I_custom[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of CustomIndexPQ with faiss IndexPQ\n",
    "- Slightly different neighbors but generally the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    }
   ],
   "source": [
    "d, m, nbits = 128, 8, 8\n",
    "index_pq_compressed = faiss.IndexPQ(d, m, nbits)\n",
    "index_pq_compressed.train(feature_list_compressed)\n",
    "index_pq_compressed.add(feature_list_compressed)\n",
    "D, I = index_pq_compressed.search(random_feature_list_compressed, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[12010.435, 13992.476, 14116.026, 14533.341, 15198.48 ],\n",
       "        [ 9567.162, 11588.168, 12361.848, 13200.58 , 13280.393],\n",
       "        [ 8834.732, 14526.405, 14526.405, 14662.773, 14812.835],\n",
       "        [16673.625, 18272.426, 19130.832, 21897.4  , 22467.85 ],\n",
       "        [13159.116, 15042.82 , 15119.03 , 16003.898, 16171.402]],\n",
       "       dtype=float32),\n",
       " array([[8115, 8141, 8108, 8154, 8136],\n",
       "        [7977, 8002, 8027, 8028, 8006],\n",
       "        [ 583,  580, 1015,  703,  587],\n",
       "        [7474, 7472, 7473, 7515, 7500],\n",
       "        [5807, 5849, 5846, 5865, 5847]]))"
      ]
     },
     "execution_count": 829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[:5],I[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 for : 0.67\n",
      "Recall@5 for : 0.67\n"
     ]
    }
   ],
   "source": [
    "compare(I_flat_compressed,random_feature_list_compressed,index_pq_compressed)\n",
    "compare(I_flat_compressed,random_feature_list_compressed,custom)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using more difficult VOC2012 data\n",
    "- Custom code does worse than faiss 0.06 vs 0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_feature_list = pickle.load(open('features/features-voc2012-resnet.pickle','rb'))\n",
    "pca = PCA(n_components=128)\n",
    "voc_feature_list_compressed = pca.fit_transform(voc_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_indices = random.sample(range(0, len(voc_feature_list)), 100)\n",
    "voc_random_feature_list = np.array([voc_feature_list[each_index] for each_index in random_image_indices])\n",
    "voc_random_feature_list_compressed = np.array([voc_feature_list_compressed[each_index] for each_index in random_image_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ground truth\n",
    "D_flat_compressed, I_flat_compressed = index_compressed.search(voc_random_feature_list_compressed, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 for : 0.13\n",
      "Recall@5 for : 0.06\n"
     ]
    }
   ],
   "source": [
    "compare(I_flat_compressed,voc_random_feature_list_compressed,index_pq_compressed)\n",
    "compare(I_flat_compressed,voc_random_feature_list_compressed,custom)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom IndexIVFPQ\n",
    "- Explanation of PQ and IVFPQ at https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf\n",
    "\n",
    "**Differences between my implementation and paper**\n",
    "- My inverted index did not have `{coarse_centroid: [(id1, code1),...]}` structure like in paper, but more simply `{coarse_centroid:[id1,id2,...]}` and another array of codes because this data model is easier to handle (need to learn C++, SIMD first to appreciate the storage patterns for high performance compute)\n",
    "\n",
    "**Problems**\n",
    "- Kmeans coarse clustering too slow (36 secs for 9k images, 128 dim), there must be faster way\n",
    "- Search over 9k images, 128 dim takes 20 seconds, still slow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Add"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating coarse quantizer and residual database vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def fit_coarse_quantizer(feature_list):\n",
    "    d = 128 # dimension\n",
    "    m=8\n",
    "    nlist = 100\n",
    "    nbits = 8\n",
    "    k=2**nbits\n",
    "\n",
    "    coarse_quantizer = KMeans(n_clusters=nlist, random_state=1).fit(feature_list)\n",
    "    feature_list_residual = feature_list - coarse_quantizer.cluster_centers_[coarse_quantizer.labels_]  # generate residual database vectors to be fine quantized\n",
    "    return feature_list_residual, coarse_quantizer\n",
    "\n",
    "feature_list_residual, coarse_quantizer = fit_coarse_quantizer(feature_list_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_coarse_quantizer(feature_list):\n",
    "    \"\"\"Find closest IVF centroid using coarse quantizer and get residuals\"\"\"\n",
    "    ivf_cells = coarse_quantizer.predict(feature_list)\n",
    "    feature_list_residual = feature_list - coarse_quantizer.cluster_centers_[ivf_cells]  # generate residual database vectors to be fine quantized\n",
    "    return feature_list_residual, coarse_quantizer, ivf_cells\n",
    "\n",
    "feature_list_residual, coarse_quantizer, ivf_cells = apply_coarse_quantizer(feature_list_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def fit_fine_quantizer(feature_list_residual, d, m):\n",
    "    ds = d//m\n",
    "\n",
    "    fine_quantizers = [KMeans(n_clusters=k, random_state=1) for _ in range(m)]\n",
    "\n",
    "    for i in range(m):\n",
    "        X_i = feature_list_residual[:, i * ds : (i + 1) * ds]\n",
    "        fine_quantizers[i].fit(X_i)\n",
    "    return fine_quantizers\n",
    "\n",
    "fine_quantizers = fit_fine_quantizer(feature_list_residual, 128, 8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Quantizing residuals into codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_residuals(feature_list_residual, estimators, m): \n",
    "    n, d= feature_list_residual.shape\n",
    "    ds = d//m\n",
    "    codes = np.empty((n, m), dtype=np.uint8)  #Prevents automatic 'float64' causing IndexError: arrays used as indices must be of integer (or boolean) type\n",
    "\n",
    "    for i in range(m):\n",
    "        estimator = fine_quantizers[i]\n",
    "        X_i = feature_list_residual[:, i * ds : (i + 1) * ds]\n",
    "        codes[:, i] = estimator.predict(X_i)  # shape n number of vectors, m segments\n",
    "        \n",
    "    return codes\n",
    "\n",
    "codes = quantize_residuals(feature_list_residual, fine_quantizers, m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning residuals to inverted list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = 0\n",
    "codes_db = np.empty((0,m),dtype=np.int8) # always cleared in jupyter for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 103\n",
      "93 162\n",
      "87 113\n",
      "77 113\n",
      "3 98\n",
      "25 97\n",
      "83 52\n",
      "28 84\n",
      "81 52\n",
      "49 130\n",
      "94 150\n",
      "52 70\n",
      "65 103\n",
      "12 91\n",
      "75 68\n",
      "43 122\n",
      "98 78\n",
      "4 121\n",
      "6 49\n",
      "40 115\n",
      "62 118\n",
      "9 132\n",
      "99 60\n",
      "42 87\n",
      "96 67\n",
      "97 51\n",
      "21 51\n",
      "64 45\n",
      "7 104\n",
      "27 117\n",
      "84 105\n",
      "41 120\n",
      "32 54\n",
      "35 76\n",
      "67 183\n",
      "34 48\n",
      "14 144\n",
      "86 124\n",
      "46 76\n",
      "72 89\n",
      "33 70\n",
      "29 40\n",
      "10 174\n",
      "85 65\n",
      "55 103\n",
      "91 39\n",
      "73 85\n",
      "38 57\n",
      "20 104\n",
      "50 88\n",
      "95 78\n",
      "89 161\n",
      "79 141\n",
      "2 113\n",
      "66 46\n",
      "78 43\n",
      "24 89\n",
      "23 97\n",
      "44 108\n",
      "30 129\n",
      "5 151\n",
      "37 135\n",
      "69 115\n",
      "74 87\n",
      "57 125\n",
      "82 112\n",
      "92 116\n",
      "8 166\n",
      "15 153\n",
      "58 148\n",
      "18 69\n",
      "60 53\n",
      "17 101\n",
      "70 92\n",
      "68 46\n",
      "26 56\n",
      "13 85\n",
      "16 123\n",
      "39 68\n",
      "48 53\n",
      "51 73\n",
      "53 106\n",
      "88 95\n",
      "47 65\n",
      "71 46\n",
      "0 71\n",
      "56 92\n",
      "76 67\n",
      "61 55\n",
      "36 43\n",
      "19 120\n",
      "63 67\n",
      "22 94\n",
      "54 41\n",
      "90 37\n",
      "31 74\n",
      "80 62\n",
      "45 96\n",
      "11 70\n",
      "59 64\n"
     ]
    }
   ],
   "source": [
    "def add_inverted_list(ivf_cells, codes, codes_db):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    inverted_list = defaultdict(list)\n",
    "\n",
    "    for idx, coarse_center in enumerate(ivf_cells):  \n",
    "        inverted_list[coarse_center].append(idx)\n",
    "    \n",
    "    codes_db = np.vstack([codes_db, codes])\n",
    "    \n",
    "    return inverted_list\n",
    "\n",
    "inverted_list = add_inverted_list(ivf_cells, codes, codes_db)\n",
    "\n",
    "# useful for checking correct number of candidates got extracted from probing during search\n",
    "for key, value in inverted_list.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.1867094e+01, -9.8064718e+00, -1.1358043e+01, -2.0601606e+01,\n",
       "         7.6451123e-01,  2.9910746e+01, -1.1899613e+01, -1.1767055e+01,\n",
       "         1.0866048e+01,  6.0190296e+01,  1.4909669e+00,  5.9952347e+01,\n",
       "        -1.3209792e+01, -4.8219257e+01, -2.7503124e+01, -9.1865454e+00,\n",
       "        -9.0401993e+00,  7.2042956e+00, -2.8980122e+01,  6.1889820e+00,\n",
       "         3.3465500e+01, -3.0763933e+01,  7.2214108e+00,  2.8699830e+00,\n",
       "         7.2017416e-02, -1.1203322e+01,  1.0475516e+01, -1.0316180e+01,\n",
       "        -1.2320327e+01,  1.3685087e+01,  3.2958090e+00,  1.9592484e+00,\n",
       "        -8.7479895e-01, -1.2692191e+01, -9.5121059e+00, -1.8125988e+01,\n",
       "        -9.3066320e+00, -5.9284525e+00,  1.7693888e+01,  1.6936089e+01,\n",
       "         3.7598002e+00, -7.9874825e+00, -1.5846498e+01,  7.4989166e+00,\n",
       "         5.4958344e+00,  4.6651011e+00, -2.8053744e+00, -2.7807868e+00,\n",
       "         3.2602324e+00,  1.4717191e+00,  2.6552942e+00, -9.8519087e+00,\n",
       "        -8.2460146e+00, -1.1753324e+01, -7.9538870e+00, -8.1769524e+00,\n",
       "         1.4464678e-01, -3.3876233e+00, -4.1767683e+00, -2.2410972e+00,\n",
       "         5.1208873e+00, -2.0450871e+00, -4.7148696e-01,  9.2444420e+00,\n",
       "        -5.5773444e+00,  7.0359427e-01, -2.7734814e-02, -2.5746908e+01,\n",
       "         9.3310976e+00,  4.8698258e+00, -1.2199526e+01, -1.4147100e+01,\n",
       "        -2.3087578e+00,  8.9269562e+00,  2.4755976e+00,  4.3064833e+00,\n",
       "        -4.4861574e+00,  2.5881197e+00,  5.7878405e-01,  4.2760634e+00,\n",
       "        -3.9156208e+00,  1.3441751e+01, -7.4368734e+00, -5.1289159e-01,\n",
       "        -4.4442430e+00,  2.7028637e+00,  1.6469624e+00,  2.0412474e+00,\n",
       "         1.9081650e+01,  7.5438112e-01, -1.7771792e-01, -1.4425305e+01,\n",
       "        -1.4433537e+00,  9.5529051e+00, -1.3919681e+01,  4.0147638e+00,\n",
       "        -6.6940026e+00, -7.1499453e+00, -8.2577295e+00,  3.8894527e+00,\n",
       "         8.1986704e+00,  7.1831694e+00,  1.4111338e+01, -5.8548565e+00,\n",
       "         6.1473689e+00, -1.0703857e+00, -8.3435729e-02,  3.5703154e+00,\n",
       "        -4.4844532e+00,  7.8380575e+00,  3.1550069e+00, -1.5096399e+01,\n",
       "         1.4866409e+00,  1.2276639e+01,  6.1746855e+00,  6.1633391e+00,\n",
       "         1.1225368e+01,  4.1476550e+00,  5.2879328e-01,  9.7266936e-01,\n",
       "         2.0004311e+00,  3.5308387e+00,  3.0566306e+00, -8.0495586e+00,\n",
       "         5.0556984e+00,  4.7072501e+00, -5.2732658e+00,  2.5605944e-01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 1236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = feature_list_compressed[[0]] #  2d to because euclidean_distances expects that\n",
    "query\n",
    "nprobe = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distance of query vector (not residual) to all coarse centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_IVFcentroids(query, coarse_quantizer, nprobe):\n",
    "    query_distance_to_coarse_centroids = euclidean_distances(query, coarse_quantizer.cluster_centers_, squared=True)[0]\n",
    "    nearest_inverted_keys = np.argsort(query_distance_to_coarse_centroids)[:nprobe]  # argsort gives index of closest coarse centroids, to be filtered by nprobe\n",
    "    return nearest_inverted_keys \n",
    "\n",
    "nearest_inverted_keys = distance_to_IVFcentroids(query, coarse_quantizer, nprobe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing calculations in 1 cell (assuming nprobe=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating query residual vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest coarse centroid:  1\n"
     ]
    }
   ],
   "source": [
    "print('Closest coarse centroid: ',nearest_inverted_keys[0])\n",
    "\n",
    "def generate_query_residual(query, coarse_quantizer, current_cell):\n",
    "      # closest from coarse quantizer\n",
    "    query_residual = query - coarse_quantizer.cluster_centers_[current_cell]  # generate residual query to be compared against all quantized residuals\n",
    "    # print(query_residual.shape, query_residual[:3])\n",
    "    \n",
    "    return query_residual\n",
    "\n",
    "current_cell = nearest_inverted_keys[0]\n",
    "query_residual = generate_query_residual(query, coarse_quantizer, current_cell)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating distance table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_table(query_residual, fine_quantizers):\n",
    "    distance_table = np.empty((m, k), dtype=np.float32)  # shape m segments, distance to k clusters \n",
    "\n",
    "    d = query_residual.shape[1]\n",
    "    ds = d//m\n",
    "    for i in range(m):\n",
    "        X_i = query_residual[:, i * ds : (i + 1) * ds]\n",
    "        centers = fine_quantizers[i].cluster_centers_  # (k, ds)\n",
    "        distance_table[i, :] = euclidean_distances(X_i, centers, squared=True)\n",
    "    return distance_table\n",
    "distance_table = compute_distance_table(query_residual, fine_quantizers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering residual vectors using inverted list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_residual_vectors(inverted_list, codes, current_cell):\n",
    "    \n",
    "    filtered_ids = inverted_list[current_cell]\n",
    "    filtered_result = codes[filtered_ids]\n",
    "    # print(filtered_result.shape, filtered_result[:3])\n",
    "    \n",
    "    return filtered_result, filtered_ids\n",
    "\n",
    "current_cell = nearest_inverted_keys[0]\n",
    "\n",
    "filtered_result, filtered_ids = filter_residual_vectors(inverted_list, codes, current_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating distances on filtered residual vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances(filtered_result, distance_table):\n",
    "    distances = np.zeros(len(filtered_result), dtype=np.float32)\n",
    "\n",
    "    for i in range(m):\n",
    "        distances += distance_table[i, filtered_result[:, i]]\n",
    "    # print(distances.shape, distances[:3])\n",
    "    \n",
    "    return distances\n",
    "\n",
    "distances = calculate_distances(filtered_result, distance_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8188.985, 15658.488, 16366.43, 17327.34, 17658.04, 17810.049),\n",
       " (0, 25, 168, 470, 229, 364))"
      ]
     },
     "execution_count": 1253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_smallest_k(distances, filtered_ids, k_nearest):\n",
    "    import heapq\n",
    "    import operator\n",
    "\n",
    "    distance_id = zip(distances,filtered_ids)\n",
    "    D, I = zip(*heapq.nsmallest(k_nearest, distance_id, operator.itemgetter(0)))\n",
    "    \n",
    "    return D, I\n",
    "\n",
    "k_nearest = 6\n",
    "D, I = find_smallest_k(distances, filtered_ids, k_nearest)   # generators can only be used once! Re-run previous cell to regenerate distance_id if needed\n",
    "D, I"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat above steps for all query vectors, and all nprobe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, \n",
    "                  coarse_quantizer,\n",
    "                  nprobe,\n",
    "                  codes,\n",
    "                  k_nearest\n",
    "                  ):\n",
    "    \n",
    "    nearest_inverted_keys = distance_to_IVFcentroids(query, coarse_quantizer, nprobe)\n",
    "    \n",
    "    nprobe_distances = np.array([], dtype=np.float32)\n",
    "    nprobe_filtered_ids = np.array([], dtype=np.uint64)\n",
    "    \n",
    "    for current_cell in nearest_inverted_keys:\n",
    "        query_residual = generate_query_residual(query, coarse_quantizer, current_cell)\n",
    "        distance_table = compute_distance_table(query_residual, fine_quantizers)\n",
    "        filtered_result, filtered_ids = filter_residual_vectors(inverted_list, codes, current_cell)\n",
    "        distances = calculate_distances(filtered_result, distance_table)\n",
    "        nprobe_distances = np.append(nprobe_distances, distances)\n",
    "        nprobe_filtered_ids = np.append(nprobe_filtered_ids, filtered_ids)\n",
    "        \n",
    "    D, I = find_smallest_k(nprobe_distances, nprobe_filtered_ids, k_nearest)\n",
    "    \n",
    "    return D, I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing how recall varies with nprobe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nearest = 6\n",
    "m, k = 8, 2**nbits\n",
    "\n",
    "nprobe_test = {}\n",
    "# nprobes_to_test = range(1,4)\n",
    "nprobes_to_test = [1]\n",
    "\n",
    "for nprobe in nprobes_to_test:\n",
    "    D_list = []\n",
    "    I_list = []\n",
    "    for query in feature_list_compressed:\n",
    "        D, I = search(query.reshape(1,-1),   #sklearn euclidean_distances needs 2D\n",
    "                            coarse_quantizer,\n",
    "                            nprobe,\n",
    "                            codes,\n",
    "                            k_nearest)\n",
    "        D_list.append(D)\n",
    "        I_list.append(I)\n",
    "        \n",
    "    D_list = np.array(D_list, dtype=np.float32)\n",
    "    I_list = np.array(I_list, dtype=np.uint64)\n",
    "    \n",
    "    nprobe_test[nprobe] = (D_list, I_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 for Custom IndexIVFPQ nprobe=1: 0.6917104111986002\n"
     ]
    }
   ],
   "source": [
    "I_list = nprobe_test[1][1]\n",
    "print(f'Recall@5 for Custom IndexIVFPQ nprobe={nprobe}:', (I_list[:,1:6] == I_flat[:,[1]]).sum()/len(feature_list_compressed))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Custom IVFPQ with faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "d = 128 # dimension\n",
    "nlist = 100\n",
    "m=8\n",
    "nbits = 8\n",
    "\n",
    "quantizer = faiss.IndexFlatL2(d)  \n",
    "index = faiss.IndexIVFPQ(quantizer, d, nlist, m, nbits)\n",
    "                                  \n",
    "index.train(feature_list_compressed)\n",
    "index.add(feature_list_compressed)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 8574.242 , 15943.098 , 16497.477 , 16979.555 , 17299.773 ],\n",
       "        [10805.41  , 18739.096 , 19096.94  , 20095.824 , 20404.768 ],\n",
       "        [ 2738.3652,  4321.1904,  4330.2686,  4420.0005,  4439.327 ],\n",
       "        [ 7315.83  , 12940.91  , 13020.773 , 14139.178 , 15500.04  ],\n",
       "        [10180.457 , 18115.746 , 18790.076 , 20177.752 , 20539.121 ]],\n",
       "       dtype=float32),\n",
       " array([[   0,  168,    8,  364,   25],\n",
       "        [   1,  213,   54, 7960,   83],\n",
       "        [   2,  226,  431,  401,  306],\n",
       "        [   3,  208,   75, 6222, 3499],\n",
       "        [   4, 6028, 2448, 2419, 6021]]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_nearest = 5\n",
    "D, I = index.search(feature_list_compressed, k_nearest)\n",
    "D[:5], I[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3.9062500e-03, 1.5674359e+04, 1.7318316e+04, 2.2993758e+04,\n",
       "         2.3135705e+04],\n",
       "        [0.0000000e+00, 2.1488664e+04, 2.4966252e+04, 2.5156342e+04,\n",
       "         2.6139074e+04],\n",
       "        [0.0000000e+00, 3.6603906e+03, 3.8096055e+03, 4.0871562e+03,\n",
       "         4.8052266e+03],\n",
       "        [3.9062500e-03, 1.2588488e+04, 1.3350793e+04, 1.3417449e+04,\n",
       "         1.4603053e+04],\n",
       "        [0.0000000e+00, 2.2566000e+04, 2.3659629e+04, 2.3905688e+04,\n",
       "         2.4714777e+04]], dtype=float32),\n",
       " array([[   0,    8,  168, 7939,    6],\n",
       "        [   1,  149,  344,  213,  309],\n",
       "        [   2,  138,  431,  157,  306],\n",
       "        [   3,  426,  208,   75, 6222],\n",
       "        [   4, 5403, 2562, 6021, 2533]]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_flat = faiss.IndexFlatL2(d)  \n",
    "index_flat.add(feature_list_compressed)\n",
    "D_flat, I_flat = index_flat.search(feature_list_compressed, k_nearest)\n",
    "D_flat[:5], I_flat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 for IndexIVFPQ (compressed): 0.6674321959755031\n",
      "Recall@5 for Custom IndexIVFPQ nprobe=1: 0.6917104111986002\n",
      "Recall@5 for Custom IndexIVFPQ nprobe=2: 0.7362204724409449\n",
      "Recall@5 for Custom IndexIVFPQ nprobe=3: 0.7469378827646544\n"
     ]
    }
   ],
   "source": [
    "compare(I_flat, feature_list_compressed, index, 'IndexIVFPQ (compressed)')\n",
    "for nprobe, (D_list, I_list) in nprobe_test.items():\n",
    "    print(f'Recall@5 for Custom IndexIVFPQ nprobe={nprobe}:', (I_list[:,1:6] == I_flat[:,[1]]).sum()/len(feature_list_compressed))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation correctness\n",
    "\n",
    "- Custom IVFPQ implementation looks right based on high Recall@5 (even higher than faiss)\n",
    "- Increasing nprobe from 1 to 2 to 3 shows increasing number of true nearest neighbors found in top 5 neighbors (excluding self)\n",
    "- Time to train and search is slow though"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packing functions into a class to simplify method signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1381,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import heapq\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "class CustomIndexIVFPQ:\n",
    "    \n",
    "    BITS2DTYPE = {\n",
    "        8: np.uint8,\n",
    "        16: np.uint16,\n",
    "    }\n",
    "    \n",
    "    def __init__(self,\n",
    "                 d: int,\n",
    "                 m: int,\n",
    "                 nlist: int,\n",
    "                 nbits: int,) -> None:\n",
    "        \"\"\"Custom IndexIVFPQ implementation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        d\n",
    "            Dimensionality of the original vectors.\n",
    "        m\n",
    "            Number of segments.\n",
    "        nlist\n",
    "            Number of coarse centroids for IVF\n",
    "        nbits\n",
    "            Number of bits.\n",
    "        \"\"\"\n",
    "        if d % m != 0:\n",
    "            raise ValueError(\"d needs to be a multiple of m\")\n",
    "\n",
    "        if nbits not in CustomIndexIVFPQ.BITS2DTYPE:\n",
    "            raise ValueError(f\"Unsupported number of bits {nbits}\")\n",
    "\n",
    "        self.m = m\n",
    "        self.nlist = nlist\n",
    "        self.nprobe = 1\n",
    "        self.k = 2**nbits\n",
    "        self.d = d\n",
    "        self.ds = d // m\n",
    "\n",
    "        self.coarse_quantizer = KMeans(n_clusters=self.nlist, random_state=1)\n",
    "        self.inverted_list = defaultdict(list)\n",
    "        self.max_id = 0 # to start following batches of vector adding from the right index to prevent duplicate ids if in different IVF cell or overwriting of data if in same IVF cell\n",
    "        self.codes_db = np.empty((0,m),dtype=CustomIndexIVFPQ.BITS2DTYPE[nbits]) # always cleared in jupyter for convenience\n",
    "        \n",
    "        self.fine_quantizers = [KMeans(n_clusters=self.k, random_state=1) for _ in range(m)]\n",
    "\n",
    "        self.is_trained = False\n",
    "\n",
    "        self.dtype = CustomIndexIVFPQ.BITS2DTYPE[nbits]\n",
    "        self.dtype_orig = np.float32\n",
    "        \n",
    "    def fit_coarse_quantizer(self, feature_list):\n",
    "        \"\"\"Coarse quantizer to divide database vectors into various voronoi cells so search only probes nprobe closest\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_list\n",
    "            Data to be converted to residuals\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        feature_list_residual\n",
    "            Residuals created by subtracting each vector with it's own coarse centroid\n",
    "        \"\"\"\n",
    "        self.coarse_quantizer.fit(feature_list)\n",
    "        feature_list_residual = feature_list - self.coarse_quantizer.cluster_centers_[self.coarse_quantizer.labels_]  # generate residual database vectors to be fine quantized\n",
    "        return feature_list_residual\n",
    "    \n",
    "    def fit_fine_quantizer(self, feature_list_residual):\n",
    "        \"\"\"Fit m fine quantizers for each of m segments\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_list_residual\n",
    "            Residuals created by subtracting each vector with it's own coarse centroid\n",
    "        \"\"\"\n",
    "        for i in range(self.m):\n",
    "            X_i = feature_list_residual[:, i * self.ds : (i + 1) * self.ds]\n",
    "            self.fine_quantizers[i].fit(X_i)\n",
    "    \n",
    "    def apply_coarse_quantizer(self, feature_list):\n",
    "        \"\"\"Find closest IVF centroid using coarse quantizer and get residuals\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_list\n",
    "            Raw Vectors to be assigned to coarse quantizer centroids\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        feature_list_residual\n",
    "            Residuals to be quantized by fine quantizer after coarse quantization\n",
    "        \n",
    "        ivf_cells\n",
    "            labels of which coarse centroids each vector goes into\n",
    "        \"\"\"\n",
    "        ivf_cells = self.coarse_quantizer.predict(feature_list)\n",
    "        feature_list_residual = feature_list - self.coarse_quantizer.cluster_centers_[ivf_cells]  # generate residual database vectors to be fine quantized\n",
    "        return feature_list_residual, ivf_cells\n",
    "            \n",
    "    def quantize_residuals(self, feature_list_residual):\n",
    "        \"\"\"\n",
    "        Fine quantization of residuals of both database and query vectors\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_list_residual\n",
    "            Residuals created by subtracting each vector with it's own coarse centroid\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        codes\n",
    "            Quantized codes of residuals, shaped n, m  \n",
    "        \"\"\" \n",
    "        n = len(feature_list_residual)\n",
    "        codes = np.empty((n, self.m), dtype=self.dtype)  #Prevents automatic 'float64' causing IndexError: arrays used as indices must be of integer (or boolean) type\n",
    "\n",
    "        for i in range(self.m):\n",
    "            estimator = self.fine_quantizers[i]\n",
    "            X_i = feature_list_residual[:, i * self.ds : (i + 1) * self.ds]\n",
    "            codes[:, i] = estimator.predict(X_i)  # shape n number of vectors, m segments\n",
    "            \n",
    "        return codes\n",
    "    \n",
    "    def add_inverted_list(self, ivf_cells, codes):\n",
    "        \"\"\"\n",
    "        Assign ids to cells and add codes to database\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        ivf_cells\n",
    "            coarse quantized labels of vectors\n",
    "        codes\n",
    "            Quantized codes of residuals to be added to database of quantized vectors\n",
    "        \"\"\"\n",
    "        for idx, coarse_center in enumerate(ivf_cells, start=self.max_id):  \n",
    "            self.inverted_list[coarse_center].append(idx)\n",
    "        \n",
    "        self.max_id += len(codes) # update max_id so next addition to IVF don't duplicate id (if same different coarse_center) or overwrite data (if same coarse_center)\n",
    "        self.codes_db = np.vstack([self.codes_db, codes])\n",
    "    \n",
    "    def distance_to_IVFcentroids(self, query):\n",
    "        \"\"\"\n",
    "        Find distance of raw query to coarse centroids\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        query\n",
    "            Raw query vector\n",
    "        \n",
    "        \"\"\"\n",
    "        query_distance_to_coarse_centroids = euclidean_distances(query, self.coarse_quantizer.cluster_centers_, squared=True)[0]\n",
    "        nearest_inverted_keys = np.argsort(query_distance_to_coarse_centroids)[:self.nprobe]  # argsort gives index of closest coarse centroids, to be filtered by nprobe\n",
    "        return nearest_inverted_keys \n",
    "\n",
    "    def generate_query_residual(self, query, current_cell):\n",
    "        \"\"\"Find closest IVF centroid and return residual of query from that centroid\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        query\n",
    "            Raw query vector\n",
    "        current_cell\n",
    "            A particular coarse centroid explored during probing for IVF cells\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        query_residual = query - self.coarse_quantizer.cluster_centers_[current_cell]  # generate residual query to be compared against all quantized residuals\n",
    "        \n",
    "        return query_residual\n",
    "    \n",
    "    def compute_distance_table(self, query_residual):\n",
    "        \"\"\"Distance table per coarse centroid for reuse by all quantized residual vectors in same cell \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query_residual\n",
    "            Residual of query vector\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        distance_table \n",
    "            Table of distances from each query vector to all k clusters, for each segment, to be used by all database vectors in same coarse centroid\n",
    "        \"\"\"\n",
    "        distance_table = np.empty((self.m, self.k), dtype=self.dtype_orig)  # shape m segments, distance to k clusters \n",
    "\n",
    "        for i in range(self.m):\n",
    "            X_i = query_residual[:, i * self.ds : (i + 1) * self.ds]\n",
    "            centers = self.fine_quantizers[i].cluster_centers_  # (k, ds)\n",
    "            distance_table[i, :] = euclidean_distances(X_i, centers, squared=True)\n",
    "        return distance_table\n",
    "    \n",
    "    def filter_residual_vectors(self, current_cell):\n",
    "        \"\"\"Identify only relevant vectors in same cell as query to compute distances for\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        current_cell\n",
    "            particular coarse centroid explored during probing for IVF cells\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        filtered_result\n",
    "            Filtered codes so search only calculates their distances\n",
    "        filtered_ids\n",
    "            Filtered ids used for indicating which vectors are being searched\n",
    "        \"\"\"\n",
    "    \n",
    "        filtered_ids = self.inverted_list[current_cell]\n",
    "        filtered_result = self.codes_db[filtered_ids]\n",
    "        \n",
    "        return filtered_result, filtered_ids\n",
    "    \n",
    "    def calculate_distances(self, filtered_result, distance_table):\n",
    "        \"\"\"Calculate distance of each database vector with quantized query vector\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        filtered_result\n",
    "            Filtered codes for calculating distances with their coarse centroid \n",
    "        distance_table\n",
    "            Table of distances from each query vector to all k clusters, for each segment, to be used by all database vectors in same coarse centroid\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        distances\n",
    "            Distance of each database vector with quantized query vector\n",
    "        \"\"\"\n",
    "        distances = np.zeros(len(filtered_result), dtype=self.dtype_orig)\n",
    "\n",
    "        for i in range(m):\n",
    "            distances += distance_table[i, filtered_result[:, i]]\n",
    "        \n",
    "        return distances\n",
    "    \n",
    "    def find_smallest_k(self, distances, filtered_ids, k_nearest):\n",
    "        \"\"\"Find nearest k neighbors (including self)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        distances\n",
    "            Distance of each database vector with quantized query vector\n",
    "        filtered_ids\n",
    "            ids of vectors\n",
    "        k_nearest\n",
    "            Number of approximate neighbors\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        D\n",
    "            K nearest distances per query vector\n",
    "        I\n",
    "            Indices of the K nearest distances per query vector\n",
    "        \"\"\"\n",
    "        distance_id = zip(distances,filtered_ids)\n",
    "        D, I = zip(*heapq.nsmallest(k_nearest, distance_id, operator.itemgetter(0)))\n",
    "        \n",
    "        return D, I\n",
    "\n",
    "    def train(self, feature_list: np.ndarray) -> None:\n",
    "        \"\"\"Train the index given data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_list\n",
    "            Array of shape `(n, d)` and dtype `float32`.\n",
    "        \"\"\"\n",
    "        feature_list_residual = self.fit_coarse_quantizer(feature_list)\n",
    "        self.fit_fine_quantizer(feature_list_residual)\n",
    "        \n",
    "        self.is_trained = True\n",
    "\n",
    "    def add(self, feature_list: np.ndarray) -> None:\n",
    "        \"\"\"Add vectors to the database (their encoded versions).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_list\n",
    "            Array of shape `(n_codes, d)` of dtype `np.float32`.\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            Cannot add data if quantizers not trained\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Both coarse and fine quantizers need to be trained first.\")\n",
    "    \n",
    "        feature_list_residual, ivf_cells = self.apply_coarse_quantizer(feature_list)\n",
    "        codes = self.quantize_residuals(feature_list_residual)\n",
    "        self.add_inverted_list(ivf_cells, codes)\n",
    "\n",
    "    def search(self, query: np.ndarray, k_nearest: int) -> tuple:\n",
    "        \"\"\"Search for k nearest neighbors\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        query\n",
    "            Raw query vector\n",
    "        k_nearest\n",
    "            Number of approximate neighbors\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        D\n",
    "            K nearest distances per query vector\n",
    "        I\n",
    "            Indices of the K nearest distances per query vector\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            Cannot add data if quantizers not trained\n",
    "            Cannot search database if it's empty\n",
    "        \n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Both coarse and fine quantizers need to be trained first.\")\n",
    "        \n",
    "        if self.codes_db.size == 0:\n",
    "            raise ValueError(\"No codes detected. You need to run `add` first\")\n",
    "        \n",
    "        nearest_inverted_keys = self.distance_to_IVFcentroids(query)\n",
    "        \n",
    "        nprobe_distances = np.array([], dtype=self.dtype_orig)\n",
    "        nprobe_filtered_ids = np.array([], dtype=np.uint64)\n",
    "        \n",
    "        for current_cell in nearest_inverted_keys:\n",
    "            query_residual = self.generate_query_residual(query, current_cell)\n",
    "            distance_table = self.compute_distance_table(query_residual)\n",
    "            filtered_result, filtered_ids = self.filter_residual_vectors(current_cell)\n",
    "            distances = self.calculate_distances(filtered_result, distance_table)\n",
    "            \n",
    "            nprobe_distances = np.append(nprobe_distances, distances)\n",
    "            nprobe_filtered_ids = np.append(nprobe_filtered_ids, filtered_ids)\n",
    "            \n",
    "        D, I = self.find_smallest_k(nprobe_distances, nprobe_filtered_ids, k_nearest)\n",
    "    \n",
    "        # some will return < k neighbors, need to pad on right to form rectangular result array\n",
    "        # assigning -1 into uint8 causes 18446744073709551615, ok for evaluation as long as doesn't match a ground truth index\n",
    "        return np.pad(D,pad_width=(0,k_nearest-len(D)),constant_values=-1), np.pad(I,pad_width=(0,k_nearest-len(I)),constant_values=-1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1384,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "d = 128\n",
    "m = 8\n",
    "nlist = 100\n",
    "nbits = 8\n",
    "custom_ivfpq = CustomIndexIVFPQ(d,m,nlist,nbits)\n",
    "\n",
    "custom_ivfpq.train(feature_list_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {})"
      ]
     },
     "execution_count": 1385,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 8), dtype=uint8)"
      ]
     },
     "execution_count": 1385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_ivfpq.inverted_list\n",
    "custom_ivfpq.codes_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1386,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ivfpq.add(feature_list_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1387,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nearest = 6\n",
    "\n",
    "nprobe_test = {}\n",
    "# nprobes_to_test = range(1,4)\n",
    "nprobes_to_test = [1]\n",
    "\n",
    "for nprobe in nprobes_to_test:\n",
    "    D_list = []\n",
    "    I_list = []\n",
    "    for query in feature_list_compressed:\n",
    "        D, I = custom_ivfpq.search(query.reshape(1,-1),   #sklearn euclidean_distances needs 2D\n",
    "                                   k_nearest)\n",
    "        D_list.append(D)\n",
    "        I_list.append(I)\n",
    "        \n",
    "    D_list = np.array(D_list, dtype=np.float32)\n",
    "    I_list = np.array(I_list, dtype=np.uint64)\n",
    "    \n",
    "    nprobe_test[nprobe] = (D_list, I_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 for Custom IndexIVFPQ nprobe=1: 0.6917104111986002\n"
     ]
    }
   ],
   "source": [
    "I_list = nprobe_test[1][1]\n",
    "print(f'Recall@5 for Custom IndexIVFPQ nprobe={nprobe}:', (I_list[:,1:6] == I_flat[:,[1]]).sum()/len(feature_list_compressed))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring Custom IndexIVFPQ to switch both quantizers from sklearn to faiss.Kmeans\n",
    "- Kmeans is too slow to train (40 secs on 9k caltech)\n",
    "\n",
    "**Identifying API Changes** (preparing for find and replace)\n",
    "\n",
    "|      Sklearn      |  Faiss |\n",
    "|:-----------------:|-------:|\n",
    "|   model.fit(x) | model.train(x) |\n",
    "|   model.predict(x)|model.assign(x)[1] |\n",
    "|   model.labels_|   model.assign(x)[1] |\n",
    "|   model.cluster_centers_|  model.centroids |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proving shape equivalence between sklearn and faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanqi/code/ahrefs/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=128, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=128, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=128, random_state=1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans predict shape:  (10000,)\n",
      "Kmeans labels shape:  (10000,)\n",
      "Kmeans cluster centers shape:  (128, 100)\n",
      "Clustering 10000 points in 100D to 128 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 24 (0.21 s, search 0.19 s): objective=74839.4 imbalance=1.117 nsplit=0       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74839.4453125"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss predict shape:  (10000,)\n",
      "faiss cluster centers shape  (128, 100)\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.random((10000,100))\n",
    "ncentroids = 128\n",
    "niter = 20\n",
    "verbose = True\n",
    "d = x.shape[1]\n",
    "\n",
    "model = KMeans(n_clusters=ncentroids, random_state=1)\n",
    "model.fit(x)\n",
    "print('Kmeans predict shape: ', model.predict(x).shape)\n",
    "print('Kmeans labels shape: ', model.labels_.shape)\n",
    "print('Kmeans cluster centers shape: ', model.cluster_centers_.shape)\n",
    "\n",
    "kmeans = faiss.Kmeans(d, ncentroids, verbose=verbose, seed=1)\n",
    "kmeans.train(x) # model.fit() \n",
    "\n",
    "print('faiss predict shape: ', kmeans.assign(x)[1].shape) # model.labels_ or model.predict(x)\n",
    "print('faiss cluster centers shape ', kmeans.centroids.shape) # model.cluster_centers_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import heapq\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import faiss\n",
    "\n",
    "\n",
    "class CustomIndexIVFPQ:\n",
    "    BITS2DTYPE = {\n",
    "        8: np.uint8,\n",
    "        16: np.uint16,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d: int,\n",
    "        m: int,\n",
    "        nlist: int,\n",
    "        nbits: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Custom IndexIVFPQ implementation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d\n",
    "            Dimensionality of the original vectors.\n",
    "        m\n",
    "            Number of segments.\n",
    "        nlist\n",
    "            Number of coarse centroids for IVF\n",
    "        nbits\n",
    "            Number of bits.\n",
    "        \"\"\"\n",
    "        if d % m != 0:\n",
    "            raise ValueError(\"d needs to be a multiple of m\")\n",
    "\n",
    "        if nbits not in CustomIndexIVFPQ.BITS2DTYPE:\n",
    "            raise ValueError(f\"Unsupported number of bits {nbits}\")\n",
    "\n",
    "        self.m = m\n",
    "        self.nlist = nlist\n",
    "        self.nprobe = 1\n",
    "        self.k = 2**nbits\n",
    "        self.d = d\n",
    "        self.ds = d // m\n",
    "\n",
    "        # self.coarse_quantizer = KMeans(n_clusters=self.nlist, random_state=1)\n",
    "        self.coarse_quantizer = faiss.Kmeans(d, nlist, seed=1)\n",
    "        self.inverted_list = defaultdict(list)\n",
    "        self.max_id = 0  # to start following batches of vector adding from the right index to prevent duplicate ids if in different IVF cell or overwriting of data if in same IVF cell\n",
    "        self.codes_db = np.empty(\n",
    "            (0, m), dtype=CustomIndexIVFPQ.BITS2DTYPE[nbits]\n",
    "        ) \n",
    "\n",
    "        self.fine_quantizers = [\n",
    "            # KMeans(n_clusters=self.k, random_state=1) for _ in range(m)\n",
    "            faiss.Kmeans(self.ds, self.k, seed=1) for _ in range(m)\n",
    "        ]\n",
    "\n",
    "        self.is_trained = False\n",
    "\n",
    "        self.dtype = CustomIndexIVFPQ.BITS2DTYPE[nbits]\n",
    "        self.dtype_orig = np.float32\n",
    "\n",
    "    def fit_coarse_quantizer(self, feature_list):\n",
    "        \"\"\"Coarse quantizer to divide database vectors into various voronoi cells so search only probes nprobe closest\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_list\n",
    "            Data to be converted to residuals\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        feature_list_residual\n",
    "            Residuals created by subtracting each vector with it's own coarse centroid\n",
    "        \"\"\"\n",
    "        self.coarse_quantizer.train(feature_list)\n",
    "        feature_list_residual = (\n",
    "            feature_list\n",
    "            - self.coarse_quantizer.centroids[self.coarse_quantizer.assign(feature_list)[1]]\n",
    "        )  # generate residual database vectors to be fine quantized\n",
    "        return feature_list_residual\n",
    "\n",
    "    def fit_fine_quantizer(self, feature_list_residual):\n",
    "        \"\"\"Fit m fine quantizers for each of m segments\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_list_residual\n",
    "            Residuals created by subtracting each vector with it's own coarse centroid\n",
    "        \"\"\"\n",
    "        for i in range(self.m):\n",
    "            X_i = feature_list_residual[:, i * self.ds : (i + 1) * self.ds]\n",
    "            self.fine_quantizers[i].train(X_i)\n",
    "\n",
    "    def apply_coarse_quantizer(self, feature_list):\n",
    "        \"\"\"Find closest IVF centroid using coarse quantizer and get residuals\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_list\n",
    "            Raw Vectors to be assigned to coarse quantizer centroids\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        feature_list_residual\n",
    "            Residuals to be quantized by fine quantizer after coarse quantization\n",
    "\n",
    "        ivf_cells\n",
    "            labels of which coarse centroids each vector goes into\n",
    "        \"\"\"\n",
    "        ivf_cells = self.coarse_quantizer.assign(feature_list)[1]\n",
    "        feature_list_residual = (\n",
    "            feature_list - self.coarse_quantizer.centroids[ivf_cells]\n",
    "        )  # generate residual database vectors to be fine quantized\n",
    "        return feature_list_residual, ivf_cells\n",
    "\n",
    "    def quantize_residuals(self, feature_list_residual):\n",
    "        \"\"\"\n",
    "        Fine quantization of residuals of both database and query vectors\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_list_residual\n",
    "            Residuals created by subtracting each vector with it's own coarse centroid\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        codes\n",
    "            Quantized codes of residuals, shaped n, m\n",
    "        \"\"\"\n",
    "        n = len(feature_list_residual)\n",
    "        codes = np.empty(\n",
    "            (n, self.m), dtype=self.dtype\n",
    "        )  # Prevents automatic 'float64' causing IndexError: arrays used as indices must be of integer (or boolean) type\n",
    "\n",
    "        for i in range(self.m):\n",
    "            estimator = self.fine_quantizers[i]\n",
    "            X_i = feature_list_residual[:, i * self.ds : (i + 1) * self.ds]\n",
    "            codes[:, i] = estimator.assign(\n",
    "                X_i)[1]  # shape n number of vectors, m segments\n",
    "\n",
    "        return codes\n",
    "\n",
    "    def add_inverted_list(self, ivf_cells, codes):\n",
    "        \"\"\"\n",
    "        Assign ids to cells and add codes to database\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ivf_cells\n",
    "            coarse quantized labels of vectors\n",
    "        codes\n",
    "            Quantized codes of residuals to be added to database of quantized vectors\n",
    "        \"\"\"\n",
    "        for idx, coarse_center in enumerate(ivf_cells, start=self.max_id):\n",
    "            self.inverted_list[coarse_center].append(idx)\n",
    "\n",
    "        self.max_id += len(\n",
    "            codes\n",
    "        )  # update max_id so next addition to IVF don't duplicate id (if same different coarse_center) or overwrite data (if same coarse_center)\n",
    "        self.codes_db = np.vstack([self.codes_db, codes])\n",
    "\n",
    "    def distance_to_IVFcentroids(self, query):\n",
    "        \"\"\"\n",
    "        Find distance of raw query to coarse centroids\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query\n",
    "            Raw query vector\n",
    "\n",
    "        \"\"\"\n",
    "        query_distance_to_coarse_centroids = euclidean_distances(\n",
    "            query, self.coarse_quantizer.centroids, squared=True\n",
    "        )[0]\n",
    "        nearest_inverted_keys = np.argsort(query_distance_to_coarse_centroids)[\n",
    "            : self.nprobe\n",
    "        ]  # argsort gives index of closest coarse centroids, to be filtered by nprobe\n",
    "        return nearest_inverted_keys\n",
    "\n",
    "    def generate_query_residual(self, query, current_cell):\n",
    "        \"\"\"Find closest IVF centroid and return residual of query from that centroid\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query\n",
    "            Raw query vector\n",
    "        current_cell\n",
    "            A particular coarse centroid explored during probing for IVF cells\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        query_residual = (\n",
    "            query - self.coarse_quantizer.centroids[current_cell]\n",
    "        )  # generate residual query to be compared against all quantized residuals\n",
    "\n",
    "        return query_residual\n",
    "\n",
    "    def compute_distance_table(self, query_residual):\n",
    "        \"\"\"Distance table per coarse centroid for reuse by all quantized residual vectors in same cell\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query_residual\n",
    "            Residual of query vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        distance_table\n",
    "            Table of distances from each query vector to all k clusters, for each segment, to be used by all database vectors in same coarse centroid\n",
    "        \"\"\"\n",
    "        distance_table = np.empty(\n",
    "            (self.m, self.k), dtype=self.dtype_orig\n",
    "        )  # shape m segments, distance to k clusters\n",
    "\n",
    "        for i in range(self.m):\n",
    "            X_i = query_residual[:, i * self.ds : (i + 1) * self.ds]\n",
    "            centers = self.fine_quantizers[i].centroids  # (k, ds)\n",
    "            distance_table[i, :] = euclidean_distances(X_i, centers, squared=True)\n",
    "        return distance_table\n",
    "\n",
    "    def filter_residual_vectors(self, current_cell):\n",
    "        \"\"\"Identify only relevant vectors in same cell as query to compute distances for\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        current_cell\n",
    "            particular coarse centroid explored during probing for IVF cells\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        filtered_result\n",
    "            Filtered codes so search only calculates their distances\n",
    "        filtered_ids\n",
    "            Filtered ids used for indicating which vectors are being searched\n",
    "        \"\"\"\n",
    "\n",
    "        filtered_ids = self.inverted_list[current_cell]\n",
    "        filtered_result = self.codes_db[filtered_ids]\n",
    "\n",
    "        return filtered_result, filtered_ids\n",
    "\n",
    "    def calculate_distances(self, filtered_result, distance_table):\n",
    "        \"\"\"Calculate distance of each database vector with quantized query vector\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filtered_result\n",
    "            Filtered codes for calculating distances with their coarse centroid\n",
    "        distance_table\n",
    "            Table of distances from each query vector to all k clusters, for each segment, to be used by all database vectors in same coarse centroid\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        distances\n",
    "            Distance of each database vector with quantized query vector\n",
    "        \"\"\"\n",
    "        distances = np.zeros(len(filtered_result), dtype=self.dtype_orig)\n",
    "\n",
    "        for i in range(m):\n",
    "            distances += distance_table[i, filtered_result[:, i]]\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def find_smallest_k(self, distances, filtered_ids, k_nearest):\n",
    "        \"\"\"Find nearest k neighbors (including self)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        distances\n",
    "            Distance of each database vector with quantized query vector\n",
    "        filtered_ids\n",
    "            ids of vectors\n",
    "        k_nearest\n",
    "            Number of approximate neighbors\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        D\n",
    "            K nearest distances per query vector\n",
    "        I\n",
    "            Indices of the K nearest distances per query vector\n",
    "        \"\"\"\n",
    "        distance_id = zip(distances, filtered_ids)\n",
    "        D, I = zip(*heapq.nsmallest(k_nearest, distance_id, operator.itemgetter(0)))\n",
    "\n",
    "        return D, I\n",
    "\n",
    "    def train(self, feature_list: np.ndarray) -> None:\n",
    "        \"\"\"Train the index given data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_list\n",
    "            Array of shape `(n, d)` and dtype `float32`.\n",
    "        \"\"\"\n",
    "        feature_list_residual = self.fit_coarse_quantizer(feature_list)\n",
    "        self.fit_fine_quantizer(feature_list_residual)\n",
    "\n",
    "        self.is_trained = True\n",
    "\n",
    "    def add(self, feature_list: np.ndarray) -> None:\n",
    "        \"\"\"Add vectors to the database (their encoded versions).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_list\n",
    "            Array of shape `(n_codes, d)` of dtype `np.float32`.\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            Cannot add data if quantizers not trained\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\n",
    "                \"Both coarse and fine quantizers need to be trained first.\"\n",
    "            )\n",
    "\n",
    "        feature_list_residual, ivf_cells = self.apply_coarse_quantizer(feature_list)\n",
    "        codes = self.quantize_residuals(feature_list_residual)\n",
    "        self.add_inverted_list(ivf_cells, codes)\n",
    "\n",
    "    def search(self, query: np.ndarray, k_nearest: int) -> tuple:\n",
    "        \"\"\"Search for k nearest neighbors\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query\n",
    "            Raw query vector\n",
    "        k_nearest\n",
    "            Number of approximate neighbors\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        D\n",
    "            K nearest distances per query vector\n",
    "        I\n",
    "            Indices of the K nearest distances per query vector\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            Cannot add data if quantizers not trained\n",
    "            Cannot search database if it's empty\n",
    "\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\n",
    "                \"Both coarse and fine quantizers need to be trained first.\"\n",
    "            )\n",
    "\n",
    "        if self.codes_db.size == 0:\n",
    "            raise ValueError(\"No codes detected. You need to run `add` first\")\n",
    "\n",
    "        nearest_inverted_keys = self.distance_to_IVFcentroids(query)\n",
    "\n",
    "        nprobe_distances = np.array([], dtype=self.dtype_orig)\n",
    "        nprobe_filtered_ids = np.array([], dtype=np.uint64)\n",
    "\n",
    "        for current_cell in nearest_inverted_keys:\n",
    "            query_residual = self.generate_query_residual(query, current_cell)\n",
    "            distance_table = self.compute_distance_table(query_residual)\n",
    "            filtered_result, filtered_ids = self.filter_residual_vectors(current_cell)\n",
    "            distances = self.calculate_distances(filtered_result, distance_table)\n",
    "\n",
    "            nprobe_distances = np.append(nprobe_distances, distances)\n",
    "            nprobe_filtered_ids = np.append(nprobe_filtered_ids, filtered_ids)\n",
    "\n",
    "        D, I = self.find_smallest_k(nprobe_distances, nprobe_filtered_ids, k_nearest)\n",
    "        \n",
    "        # some will return < k neighbors, need to pad on right to form rectangular result array\n",
    "        # assigning -1 into uint8 causes 18446744073709551615, ok for evaluation as long as doesn't match a ground truth index\n",
    "        return np.pad(D,pad_width=(0,k_nearest-len(D)),constant_values=-1), np.pad(I,pad_width=(0,k_nearest-len(I)),constant_values=-1) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying correctness of refactoring\n",
    "- Recall@5 for coarse quantizer using faiss instead of sklean kmeans dropped to 0.66 from 0.69\n",
    "- Training speed improved from 40 seconds to < 2seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 9144 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    }
   ],
   "source": [
    "d = 128\n",
    "m = 8\n",
    "nlist = 100\n",
    "nbits = 8\n",
    "custom_ivfpq = CustomIndexIVFPQ(d,m,nlist,nbits)\n",
    "\n",
    "custom_ivfpq.train(feature_list_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ivfpq.add(feature_list_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nearest = 6\n",
    "\n",
    "nprobe_test = {}\n",
    "# nprobes_to_test = range(1,4)\n",
    "nprobes_to_test = [1]\n",
    "\n",
    "for nprobe in nprobes_to_test:\n",
    "    D_list = []\n",
    "    I_list = []\n",
    "    \n",
    "    for query in feature_list_compressed:\n",
    "        D, I = custom_ivfpq.search(query.reshape(1,-1),   #sklearn euclidean_distances needs 2D\n",
    "                                k_nearest)\n",
    "        D_list.append(D)   \n",
    "        I_list.append(I)\n",
    "        \n",
    "    D_list = np.array(D_list, dtype=np.float32)\n",
    "    I_list = np.array(I_list, dtype=np.uint64)\n",
    "    \n",
    "    nprobe_test[nprobe] = (D_list, I_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@5 for Custom IndexIVFPQ nprobe=1: 0.665573053368329\n"
     ]
    }
   ],
   "source": [
    "I_list = nprobe_test[1][1]\n",
    "print(f'Recall@5 for Custom IndexIVFPQ nprobe={nprobe}:', (I_list[:,1:6] == I_flat[:,[1]]).sum()/len(feature_list_compressed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de71d29f00448e1dc27ad691bf61e0c3abd829d418f3e67544fde7c932bb5501"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
